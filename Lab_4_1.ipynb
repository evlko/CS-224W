{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Gng73HMlOK9kmBfycxp6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCPnq5FQWI06"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вместо твитов про ковид будем использовать твиты известного политика, так как иначе не хватает ОЗУ в Google Colab"
      ],
      "metadata": {
        "id": "i8Eulrvk1_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('https://raw.githubusercontent.com/evlko/CS-224W/main/Data/Twitter/tweets_DT.csv')\n",
        "tweets = tweets.rename(columns={'Tweet_Text': 'text'})"
      ],
      "metadata": {
        "id": "Q0tOXQ1jWWun"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дополнительно возьмем лишь часть из них:"
      ],
      "metadata": {
        "id": "1fWsnxNVZXt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = tweets.head(1000)"
      ],
      "metadata": {
        "id": "qDJ3qXk7u_Vp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Важно, что так как мы будем решать задачу генерации текста, а не классификации, то:\n",
        "1. удалять стоп-слова не надо, так как без них будет потеряна логика языка;\n",
        "2. лематизировать не имеет смысла, так как вновь потеряется смысл."
      ],
      "metadata": {
        "id": "yhYT1_yLWcM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MFCcq43WfaB",
        "outputId": "1c4b5be8-2d53-4b2d-862e-bb75e4161517"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для удаления слов, которые начинаются с определенных символов"
      ],
      "metadata": {
        "id": "70EHW1f3Zddc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words_with_sym(text, symbol='#'):\n",
        "  return re.sub(r'{}[^\\s]*'.format(symbol), '', text)\n",
        "\n",
        "assert remove_words_with_sym('Hello, it\\'s a me, Mario!, #Top') == 'Hello, it\\'s a me, Mario!, '\n",
        "assert remove_words_with_sym('Hello, it\\'s a me, @Mario!', '@') == 'Hello, it\\'s a me, '"
      ],
      "metadata": {
        "id": "_ELgF17sWizh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полная функция для предобработки"
      ],
      "metadata": {
        "id": "na632iAVZivO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, special_symbols=['#', '@', 'http', 'pic twitter c', 'rt']):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).replace('_', '')\n",
        "    text = text.lower()\n",
        "    for special_symbol in special_symbols:\n",
        "      text = remove_words_with_sym(text, special_symbol)\n",
        "    text = [word for word in word_tokenize(text)] \n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "assert preprocess('Hello, it\\'s a me, Mario! 999') == 'hello its a me mario'"
      ],
      "metadata": {
        "id": "8efcKLIiWkD9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].apply(preprocess)"
      ],
      "metadata": {
        "id": "IkLW84trWoju"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n",
        "Проведем небольшой Exploratory Data Analysis. Узнаем:\n",
        "1. Минимальную длину твита;\n",
        "2. Среднюю;\n",
        "3. Максимальную.\n",
        "По итогу будем стараться генерировать средние твиты."
      ],
      "metadata": {
        "id": "8F-CKeVp8rdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_tweet_len, avg_tweet_len, max_tweet_len = float('inf'), 0, 0\n",
        "\n",
        "for tweet in tweets['text'].tolist():\n",
        "  tweet_len = len(tweet.split())\n",
        "  if tweet_len < min_tweet_len:\n",
        "    min_tweet_len = tweet_len\n",
        "  elif tweet_len > max_tweet_len:\n",
        "    max_tweet_len = tweet_len\n",
        "  avg_tweet_len += tweet_len\n",
        "avg_tweet_len = avg_tweet_len // (len(tweets))\n",
        "\n",
        "min_tweet_len, avg_tweet_len, max_tweet_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur1hcw5r8rC9",
        "outputId": "7c4354f4-ad36-4edd-dd80-3adf06eb43eb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 15, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "74g-QQCPXE3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем число уникальных токенов, а также сохраним их"
      ],
      "metadata": {
        "id": "RkH61ASUaZts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_words = word_tokenize(' '.join(tweets['text'].to_numpy()))\n",
        "n_words = len(text_words)\n",
        "unique_words = set(text_words)\n",
        "\n",
        "print('Total Words: %d' % n_words)\n",
        "print('Unique Words: %d' % len(unique_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjLJboLwXHnw",
        "outputId": "39c65ccb-1ed1-4ca9-ec79-1834612502a3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 15259\n",
            "Unique Words: 2850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем словарь, где ключ — слово, а значение — целое число. \n",
        "\n",
        "На будущее создадим \"обратный\" словарь. Проблем не будет, так как в данном случае мы имеем биективное отношение."
      ],
      "metadata": {
        "id": "EIPIczjM2r04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_2_index = { w: i for i, w in enumerate(unique_words) }\n",
        "index_2_word = { i: w for i, w in enumerate(unique_words) }"
      ],
      "metadata": {
        "id": "8Hsl2j8xZ2dM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка Данных\n",
        "Задача генерации — предсказать следующее слово по текущим, то есть у нас уже есть некоторая последовательность слов и мы пытаемся угадать следующее слово. Разобьем наш текст на последовательности определенной длиный и будем запоминать данные.\n",
        "\n",
        "Очевидно, что у такого подхода есть 2 проблемы:\n",
        "1. Весь текст был объединен в одну цепочку, то есть в какой-то момент твиты идут друг за другом, что нелогично, поэтому имеет смысл оперировать не всем набором слов, а отдельными твитами, каждый из которых и пытается сгенерировать свою цепочку;\n",
        "2. Подбор длины - отдельная задача, которую надо решать."
      ],
      "metadata": {
        "id": "vb65wnU13jlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "QA5FC0des3Pp"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = []\n",
        "output_words = []\n",
        "input_seq_length = avg_tweet_len-1\n",
        "\n",
        "tweets_texts = tweets['text']\n",
        "\n",
        "for tweet in tweets_texts:\n",
        "  tokenized_tweet = tweet.split()\n",
        "  if len(tokenized_tweet) < input_seq_length:\n",
        "    continue\n",
        "  for i in range(0, len(tokenized_tweet) - input_seq_length):\n",
        "    in_seq = tokenized_tweet[i:i + input_seq_length]\n",
        "    out_seq = tokenized_tweet[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])"
      ],
      "metadata": {
        "id": "WsGz1yVw5KJs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "y = to_categorical(output_words)"
      ],
      "metadata": {
        "id": "4onuWy018v6L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMwdUXw19HTS",
        "outputId": "9a865933-f04e-46c4-ce39-a73e8eaec71d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (3308, 14, 1)\n",
            "y shape: (3308, 2850)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "Hl6wnB5HvYPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим простую модель, которая будет состоять из нескольких `LSTM` слоев и дополнительных-инструментальных (`Dense`, `Dropout`, etc)"
      ],
      "metadata": {
        "id": "kCpk8fIxrCtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout"
      ],
      "metadata": {
        "id": "J_YMi-FJvaQV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBhxKdIe2S63",
        "outputId": "1f3bb7b5-0254-4868-ae9d-3cf0d9d2b97d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 14, 300)           362400    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 14, 300)           0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2850)              287850    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 810,650\n",
            "Trainable params: 810,650\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем модель:"
      ],
      "metadata": {
        "id": "Oe24JUahsvVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=100, verbose=5)"
      ],
      "metadata": {
        "id": "SaQvuhWm9SCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Генерация"
      ],
      "metadata": {
        "id": "5gf2i6d9guSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберем случайную последовательность:"
      ],
      "metadata": {
        "id": "pRh_0-xXl2DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)\n",
        "random_seq = input_sequence[random_seq_index]\n",
        "word_sequence = [index_2_word[value] for value in random_seq]\n",
        "\n",
        "' '.join(word_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "uZynsQeVgyfV",
        "outputId": "d5cc3533-22cf-40ba-cf9e-32f8064aa5f7"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'come at you from all sides they dont know how to win i will'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем продолжить твит до максимальной длины: "
      ],
      "metadata": {
        "id": "fg1iwG0tmcgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_tweet_len-len(word_sequence)+1):\n",
        "  int_sample = np.reshape(random_seq, (1, len(random_seq), 1))\n",
        "\n",
        "  predicted_words_indices = model.predict(int_sample, verbose=5)\n",
        "  predicted_word_index = np.argmax(predicted_words_indices)\n",
        "\n",
        "  random_seq.append(predicted_word_index)\n",
        "  random_seq = random_seq[1:len(random_seq)]\n",
        "\n",
        "  word_sequence.append(index_2_word[predicted_word_index])"
      ],
      "metadata": {
        "id": "K_kVqY0shZuw"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат генерации"
      ],
      "metadata": {
        "id": "GXGo5h0OnWQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(word_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "04MqlqRrhfo6",
        "outputId": "05cbc31b-fd5c-4e37-a35e-10e890a6046d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'come at you from all sides they dont know how to win i will morningjoe apologize and never statistics doubt defending will make d loyal the any even oh totally'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    }
  ]
}